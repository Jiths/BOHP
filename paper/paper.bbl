% $ biblatex auxiliary file $
% $ biblatex version 2.5 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\entry{Fanselow2016-gz}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Fanselow}{F.}%
     {Michael~S}{M.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Wassum}{W.}%
     {Kate~M}{K.~M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{FMSWKM1}
  \strng{fullhash}{FMSWKM1}
  \field{sortinit}{F}
  \field{abstract}{%
  Pavlovian conditioning is the process by which we learn relationships between
  stimuli and thus constitutes a basic building block for how the brain
  constructs representations of the world. We first review the major concepts
  of Pavlovian conditioning and point out many of the pervasive
  misunderstandings about just what conditioning is. This brings us to a modern
  redefinition of conditioning as the process whereby experience with a
  conditional relationship between stimuli bestows these stimuli with the
  ability to promote adaptive behavior patterns that did not occur before the
  experience. Working from this framework, we provide an in-depth analysis of
  two examples, fear conditioning and food-based appetitive conditioning, which
  include a description of the only partially overlapping neural circuitry of
  each. We also describe how these circuits promote the basic characteristics
  that define Pavlovian conditioning, such as error-correction-driven
  regulation of learning.%
  }
  \field{number}{1}
  \field{title}{The Origins and Organization of Vertebrate Pavlovian
  Conditioning}
  \field{volume}{8}
  \field{journaltitle}{Cold Spring Harb. Perspect. Biol.}
  \field{month}{01}
  \field{year}{2016}
\endentry

\entry{Graves2014-ch}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Graves}{G.}%
     {Alex}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Wayne}{W.}%
     {Greg}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Danihelka}{D.}%
     {Ivo}{I.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{GAWGDI1}
  \strng{fullhash}{GAWGDI1}
  \field{sortinit}{G}
  \field{abstract}{%
  We extend the capabilities of neural networks by coupling them to external
  memory resources, which they can interact with by attentional processes. The
  combined system is analogous to a Turing Machine or Von Neumann architecture
  but is differentiable end-to-end, allowing it to be efficiently trained with
  gradient descent. Preliminary results demonstrate that Neural Turing Machines
  can infer simple algorithms such as copying, sorting, and associative recall
  from input and output examples.%
  }
  \verb{eprint}
  \verb 1410.5401
  \endverb
  \field{title}{Neural Turing Machines}
  \field{eprinttype}{arXiv}
  \field{eprintclass}{cs.NE}
  \field{month}{10}
  \field{year}{2014}
\endentry

\entry{Santoro2016-jn}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Santoro}{S.}%
     {Adam}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Bartunov}{B.}%
     {Sergey}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Botvinick}{B.}%
     {Matthew}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Wierstra}{W.}%
     {Daan}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Lillicrap}{L.}%
     {Timothy}{T.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SA+1}
  \strng{fullhash}{SABSBMWDLT1}
  \field{sortinit}{S}
  \field{abstract}{%
  Despite recent breakthroughs in the applications of deep neural networks, one
  setting that presents a persistent challenge is that of ``one-shot
  learning.'' Traditional gradient-based networks require a lot of data to
  learn, often through extensive iterative training. When new data is
  encountered, the models must inefficiently relearn their parameters to
  adequately incorporate the new information without catastrophic interference.
  Architectures with augmented memory capacities, such as Neural Turing
  Machines (NTMs), offer the ability to quickly encode and retrieve new
  information, and hence can potentially obviate the downsides of conventional
  models. Here, we demonstrate the ability of a memory-augmented neural network
  to rapidly assimilate new data, and leverage this data to make accurate
  predictions after only a few samples. We also introduce a new method for
  accessing an external memory that focuses on memory content, unlike previous
  methods that additionally use memory location-based focusing mechanisms.%
  }
  \verb{eprint}
  \verb 1605.06065
  \endverb
  \field{title}{One-shot Learning with {Memory-Augmented} Neural Networks}
  \field{annotation}{%
  - Quite confusing...<div><br></div><div>- Input is downsampled, flattened
  images of characters w/ labels</div><div><br></div><div>- Controller is an
  lstm that uses input and state to produce 'memory key'. The key is used to
  retrieve a blend of memories that look most like the key, and is then written
  to memory at either the most recent or least-used location (is that
  determined by the controller or by a hard-learned
  parameter?)</div><div><br></div><div>- Somehow the system, and human, get
  better than chance accuracy in character classification on first
  presentation?? How is that possible? - Oh, yes: if you have already seen
  characters from many classes, then characters not looking like any of them
  likely to belong to the remaining class! Clever for the network to be able to
  see that...</div><div><br></div><div>- Also: how is performance from simple
  FF NN + kNN so terrible? Well, the FF is pretty bad - simple auto-encoder. A
  full CNN might do much better - though as they observe, it would have many
  more parameters..</div><div><br></div><div>- But the auto-encoder is trained
  unsupervised, while the MANN is trained supervised !... maybe unfair
  baseline...</div>%
  }
  \field{eprinttype}{arXiv}
  \field{eprintclass}{cs.LG}
  \field{year}{2016}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Sukhbaatar2015-ly}{incollection}{}
  \name{author}{4}{}{%
    {{}%
     {Sukhbaatar}{S.}%
     {Sainbayar}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Szlam}{S.}%
     {Arthur}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Weston}{W.}%
     {Jason}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Fergus}{F.}%
     {Rob}{R.}%
     {}{}%
     {}{}}%
  }
  \name{editor}{5}{}{%
    {{}%
     {Cortes}{C.}%
     {C}{C}%
     {}{}%
     {}{}}%
    {{}%
     {Lawrence}{L.}%
     {N~D}{N.~D.}%
     {}{}%
     {}{}}%
    {{}%
     {Lee}{L.}%
     {D~D}{D.~D.}%
     {}{}%
     {}{}}%
    {{}%
     {Sugiyama}{S.}%
     {M}{M}%
     {}{}%
     {}{}}%
    {{}%
     {Garnett}{G.}%
     {R}{R}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Curran Associates, Inc.}%
  }
  \strng{namehash}{SS+1}
  \strng{fullhash}{SSSAWJFR1}
  \field{sortinit}{S}
  \field{booktitle}{Advances in Neural Information Processing Systems 28}
  \field{pages}{2440\bibrangedash 2448}
  \field{title}{{End-To-End} Memory Networks}
  \field{year}{2015}
\endentry

\entry{Vasilkoski2011-ww}{inproceedings}{}
  \name{author}{8}{}{%
    {{}%
     {Vasilkoski}{V.}%
     {Zlatko}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Ames}{A.}%
     {Heather}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Chandler}{C.}%
     {Ben}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Gorchetchnikov}{G.}%
     {Anatoli}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {L\'{e}veill\'{e}}{L.}%
     {Jasmin}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Livitz}{L.}%
     {Gennady}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Mingolla}{M.}%
     {Ennio}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Versace}{V.}%
     {Massimiliano}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{VZ+1}
  \strng{fullhash}{VZAHCBGALJLGMEVM1}
  \field{sortinit}{V}
  \field{booktitle}{The 2011 International Joint Conference on Neural Networks
  ({IJCNN})}
  \field{pages}{2563\bibrangedash 2569}
  \field{title}{Review of stability properties of neural plasticity rules for
  implementation on memristive neuromorphic hardware}
  \field{year}{2011}
\endentry

\lossort
\endlossort

\endinput
