% Generated by Paperpile. Check out http://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@INPROCEEDINGS{Vasilkoski2011-ww,
title = "Review of stability properties of neural plasticity rules for implementation on memristive neuromorphic hardware",
booktitle = "The 2011 International Joint Conference on Neural Networks ({IJCNN})",
author = "Vasilkoski, Zlatko and Ames, Heather and Chandler, Ben and Gorchetchnikov, Anatoli and L\'{e}veill\'{e}, Jasmin and Livitz, Gennady and Mingolla, Ennio and Versace, Massimiliano",
pages = "2563--2569",
year =  2011,
}

@ARTICLE{Fanselow2016-gz,
title = "The Origins and Organization of Vertebrate Pavlovian Conditioning",
author = "Fanselow, Michael S and Wassum, Kate M",
affiliation = "Department of Psychology, University of California Los Angeles, Los Angeles, California 90095-1563. Department of Psychology, University of California Los Angeles, Los Angeles, California 90095-1563.",
abstract = "Pavlovian conditioning is the process by which we learn relationships between stimuli and thus constitutes a basic building block for how the brain constructs representations of the world. We first review the major concepts of Pavlovian conditioning and point out many of the pervasive misunderstandings about just what conditioning is. This brings us to a modern redefinition of conditioning as the process whereby experience with a conditional relationship between stimuli bestows these stimuli with the ability to promote adaptive behavior patterns that did not occur before the experience. Working from this framework, we provide an in-depth analysis of two examples, fear conditioning and food-based appetitive conditioning, which include a description of the only partially overlapping neural circuitry of each. We also describe how these circuits promote the basic characteristics that define Pavlovian conditioning, such as error-correction-driven regulation of learning.",
journal = "Cold Spring Harb. Perspect. Biol.",
volume =  8,
number =  1,
month =  jan,
year =  2016,
}


@ARTICLE{Graves2014-ch,
title = "Neural Turing Machines",
author = "Graves, Alex and Wayne, Greg and Danihelka, Ivo",
abstract = "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.",
month =  oct,
year =  2014,
archivePrefix = "arXiv",
primaryClass = "cs.NE",
eprint = "1410.5401"
}

@INCOLLECTION{Sukhbaatar2015-ly,
title = "{End-To-End} Memory Networks",
booktitle = "Advances in Neural Information Processing Systems 28",
author = "Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob",
editor = "Cortes, C and Lawrence, N D and Lee, D D and Sugiyama, M and Garnett, R",
publisher = "Curran Associates, Inc.",
pages = "2440--2448",
year =  2015
}

@ARTICLE{Santoro2016-jn,
title = "One-shot Learning with {Memory-Augmented} Neural Networks",
author = "Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy",
abstract = "Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of ``one-shot learning.'' Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.",
month =  "19~" # may,
year =  2016,
annote = "- Quite confusing...<div><br></div><div>- Input is downsampled, flattened images of characters w/ labels</div><div><br></div><div>- Controller is an lstm that uses input and state to produce 'memory key'. The key is used to retrieve a blend of memories that look most like the key, and is then written to memory at either the most recent or least-used location (is that determined by the controller or by a hard-learned parameter?)</div><div><br></div><div>- Somehow the system, and human, get better than chance accuracy in character classification on first presentation?? How is that possible? - Oh, yes: if you have already seen characters from many classes, then characters not looking like any of them likely to belong to the remaining class! Clever for the network to be able to see that...</div><div><br></div><div>- Also: how is performance from simple FF NN + kNN so terrible? Well, the FF is pretty bad - simple auto-encoder. A full CNN might do much better - though as they observe, it would have many more parameters..</div><div><br></div><div>- But the auto-encoder is trained unsupervised, while the MANN is trained supervised !... maybe unfair baseline...</div>",
archivePrefix = "arXiv",
primaryClass = "cs.LG",
eprint = "1605.06065"
}

@ARTICLE{Blackman2016-jx,
title = "Monkey Prefrontal Neurons Reflect Logical Operations for Cognitive Control in a Variant of the {AX} Continuous Performance Task ({AX-CPT})",
author = "Blackman, Rachael K and Crowe, David A and DeNicola, Adele L and Sakellaridi, Sofia and MacDonald, 3rd, Angus W and Chafee, Matthew V",
journal = "J. Neurosci.",
volume =  36,
number =  14,
pages = "4067--4079",
month =  "6~" # apr,
year =  2016,
keywords = "context processing; macaque; neural activity; prefrontal; primate; schizophrenia",
language = "en"
}

